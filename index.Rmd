---
output: html_document
---
<center>

Practical Machine Learning Project
===================================
© SunnyBingoMe

Saturday, July 16, 2014
</center> 

<!-- 
**consider change = to <- **
-->

<br/>

Preface & Background
---------------------------------------------------------------------------------

To ease the workload of reviewers, I will try to use as less words as possible.

The goal is to use labeled data to "predict the manner in which they did the exercise". That is, a supervised learning & predicting process. According to the training data, there are five different manners (the "classe" column). So classification is the first choice instead of regression. Here, to improve the accuracy, Random Forest is used to build the model.

The data for the project is from: <http://groupware.les.inf.puc-rio.br/har>.

<br/>

Preparing Environment
---------------------------------------------------------------------------------

**Setting working directory & seed**

```{r env, echo=TRUE}
setwd("H:/Dropbox/PML/project");
load("Random.seed.Rdata")
```
By the way, it would be better if I could remember what seed is set. So set.seed() could be used, which is easier to reset the ".Random.seed".

**Loading data**

```{r load}
training = read.csv('pml-training.csv')
testing = read.csv('pml-testing.csv')
```

<br/>

Preprocessing
---------------------------------------------------------------------------------

**Checking data summary **

`summary(training); summary(testing);`

(The output display of summary is hidden due to consuming a lot of space.)

Here, only useful factors according to the summary of testing data will be kept. The process gets rid of "NA" columns in testing data. Besides, I remove useless data for prediction such as index number ("X") and timestamps. This procedure is done for both training and testing data. 

```{r clean}
testingClean <- subset(testing, new_window == "no", select = c(user_name, num_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:magnet_forearm_z))

trainingClean <- subset(training, new_window == "no", select = c(user_name, num_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:magnet_forearm_z, classe))
```

Now, the trainingClean dataset has only one more column, "classe", when being compared with testingClean.

```{r lib}
library(caret); library(randomForest);
```

Actually, lib randomForest will be loaded automaticlly when using "rf" as the method to train().


<br/>

Modelling
---------------------------------------------------------------------------------

In this section, I will use a machine learning algorithm to predict activity quality from activity monitors.

**To use Cross validation**

The trainControl() is needed to use cross validation in train(). The "number" means how many folds should be used in the "cv" method.

```{r trainControl}
trainControl <- trainControl(method = "cv", number = 10)
```

**Then build a Model**

According to the performance of different CPUs, The procedure may need several minutes/hours to build the model. 

```{r rf}
modFit <- train(classe ~ ., data = trainingClean, method = "rf", trControl = trainControl)
```

**Let's check the model**

```{r results}
modFit; 
```

<br/>

Accuracy & Error
---------------------------------------------------------------------------------

**Cross validation accuracy**

```{r}
modFit$resample; 
```


**Expected out-of-sample error**

```{r finalModel}
modFit$finalModel; 
```

According to this final model, the expected error rate is 0.14%. [1]


<br/>

Predicting
---------------------------------------------------------------------------------

The last step is to predict the 20 test cases. 

```{r pred20}
pred20 <- predict(modFit, testingClean)
pred20
```

And the function pml_write_files() is used to generate files for each case. I modified the code, so the filenames could have a good order in both Windows and Linux. (For convenience, these files are not included in git repository.)

```{r writefile}
pml_write_files = function(x) {
    n = length(x)
    for (i in 1:n) {
        string.filename = sprintf("problem_id_%02i.txt", i)
        filename = paste0(string.filename)
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)
    }
}
pml_write_files(pred20)
```

<br/>

Discussion & Choices
---------------------------------------------------------------------------------

Here is a little more thoughts regarding the decisions in previous sections.

**Parameters**

Let's take a look at the importance of different parameters.
```{r}
varImpPlot(modFit$finalModel)
```

Previously, I was not sure if "user_name" is necessary as a individual person could have special manners when doing exercises. But to tell from the Mean Decrease Gini plot, "user_name" is not needed.

**Models**

A lot of tree-based algorithms could be used for classification problems. But this report chose to use Random Forest, which is an ensemble method making use of bootstrap-aggregating-like procedure to improve the performance of classification. [2]

**Cross Validation vs. Data Splitting**

In this project, CV is used instead of splitting by 0.7 etc. The reasons are:

1. It is required in PML by Johns Hopkins.

2. CV usually provides better performance than manual splitting. For example:

![CV vs. Splitting](http://andrewgelman.com/wp-content/uploads/2006/03/cross-validation-3.png)

See: Cross-validation vs data splitting [3].


<br/>

Afterword
---------------------------------------------------------------------------------

The source code and data of this project is also hosted on Github:

<https://github.com/SunnyBingoMe/pml_project>

Many thanks to the teachers Jeff Leek etc., and thanks to the reviewers.

<br/>

Bibliography
---------------------------------------------------------------------------------

[1] Leo Breiman, University of California, Berkeley <http://www.stat.berkeley.edu/~breiman/OOBestimation.pdf>

[2] Random Forest <http://en.wikipedia.org/wiki/Random_forest>

[3] Cross-validation vs data splitting <http://andrewgelman.com/2006/03/16/crossvalidation_1/>


<br/><br/><br/><br/>

<center>
© SunnyBingoMe, 2014
</center>
<br/>
